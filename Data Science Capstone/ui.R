shinyUI(fluidPage(
    
    titlePanel("Predict Next Word"),
    tabsetPanel(type='tab',
                tabPanel("App Main Page",
                         sidebarLayout(
                             
                             sidebarPanel(
                                 textAreaInput('userInput',label="Input your phrase here:",value=""),
                                 br(),
                                 helpText("Note:",br(),
                                          "The following predicted word will populate in % or frequency, depending on the size of the output.")),
                             
                             mainPanel(
                                 h4("Top 10 predictions based on 300k stemmed ngrams up to 4"),
                                 tableOutput('guess')
                             )
                         )
                ),
                
                tabPanel("Pre-NLP Analysis",
                         h3("N-Grams code"),
                         p(
"library(tidyverse)",br(),
"library(SnowballC)",br(),
"library(ggplot2)",br(),
"library(dplyr)",br(),
"library(rJava)",br(),
"library(RColorBrewer)",br(),
"library(quanteda)",br(),
"library(wordcloud)",br(),
"library(doParallel)",br(),
"library(R.utils)",br(),
"library(data.table)",br(),
"library(tm)",br(),
br(),
strong("# Read txt files and store data"),br(),
"blog = readLines('./en_US.blogs.txt')",br(),
"news = readLines('./en_US.news.txt')",br(),
"twitter = readLines('./en_US.twitter.txt')",br(),
br(),
strong("#split into testing and training sets"),br(),
"sub_b <- sample(length(blog), floor(length(blog) * 0.75))",br(),
"sub_t <- sample(length(news), floor(length(news) * 0.75))",br(),
"sub_n <- sample(length(twitter), floor(length(twitter) * 0.75))",br(),
"training_b <- blog[sub_b]",br(),
"testing_b <- blog[-sub_b]",br(),
"training_t <- twitter[sub_t]",br(),
"testing_t <- twitter[-sub_t]",br(),
"training_n <- news[sub_n]",br(),
"testing_n <- news[-sub_n]",br(),
br(),
strong("# Read the files lines and initialize a character vecor"),br(),
"all_text <- c(training_b, training_t, training_n)",br(),
br(),
strong("# Create corp us and tokenize"),br(),
"qcorpus <- quanteda::corpus(all_text)",br(),
br(),
strong("# Free memory"),br(),
"rm(list=setdiff(ls(), 'qcorpus'))",br(),
br(),
strong("# Dictionary of profanity words"),br(),
"badwords<-readLines('http://www.cs.cmu.edu/~biglou/resources/bad-words.txt')",br(),
br(),
"tokenized_corpus <- tokens(qcorpus, what = 'word',",br(),
"                        remove_numbers = TRUE, remove_punct = TRUE,",br(),
"                        remove_symbols = TRUE, split_hyphens = TRUE,",br(),
"                        remove_separators = TRUE, remove_url = TRUE)",br(),
br(),
strong("# Lower case the tokens."),br(),
"tokenized_corpus <- tokens_tolower(tokenized_corpus)",br(),
br(),
strong("#Remove profane words"),br(),
"tokenized_corpus <- tokens_remove(tokenized_corpus, badwords)",br(),
br(),
strong("# Creating N-gram tokens"),br(),
"bigram <- tokens_ngrams(tokenized_corpus, n = 2)",br(),
"trigram <- tokens_ngrams(tokenized_corpus, n = 3)",br(),
"fourgram <- tokens_ngrams(tokenized_corpus, n = 4)",br(),
br(),
strong("# Create Document term matrix"),br(),
"bidtm <- dfm",br(),
br(),
strong("# Consolidate and save n-grams"),br(),
"n_gram <- rbind(bigram, trigram, fourgram)",br(),
"wbr(),rite.csv(n_gram, 'en-US_n_gram.csv', row.names=FALSE)",br(),
br(),
strong("## Create our first DT models"),br(),
strong("# 1-Grams"),br(),
"uni.dfm <- dfm(tokenized_corpus)",br(),
"uni.dfmfile <- dfm_trim(uni.dfm, min_docfreq = 10, min_termfreq = 50, verbose = TRUE)",br(),
"uni.df <- as.data.frame(convert(uni.dfmfile, to = 'tripletlist')[2:3])",br(),
"uni.df <- uni.df %>%",br(),
"  group_by(feature) %>%",br(),
"  summarise(frequency = n()) %>%",br(),
"  arrange(desc(frequency))",br(),
strong("#split the word column"),br(),
"ngram1 <- data.frame(do.call('rbind', strsplit(as.character(uni.df$feature),'_',fixed=TRUE)))",br(),
"names(ngram1) <- c('w1')",br(),
"ngram1$freq <- uni.df$frequency",br(),
"rm(uni.dfmfile, uni.dfm,unigram)",br(),
br(),
strong("# 2-Grams"),br(),
"bi.dfm <- dfm(bigram)",br(),
"bi.dfmfile <- dfm_trim(bi.dfm, min_docfreq = 2, min_termfreq = 20, verbose = TRUE)",br(),
"bi.df <- as.data.frame(convert(bi.dfmfile, to = 'tripletlist')[2:3])",br(),
"bi.df <- bi.df %>%",br(),
"  group_by(feature) %>%",br(),
"  summarise(frequency = n()) %>%",br(),
"  arrange(desc(frequency))",br(),
strong("#split the word column"),br(),
"ngram2 <- data.frame(do.call('rbind', strsplit(as.character(bi.df$feature),'_',fixed=TRUE)))",br(),
"names(ngram2) <- c('w1','w2')",br(),
"ngram2$freq <- bi.df$frequency",br(),
"rm(bi.dfmfile, bi.dfm,bigram)",br(),
br(),
strong("# 3-Grams"),br(),
"tri.dfm <- dfm(trigram)",br(),
"tri.dfmfile <- dfm_trim(tri.dfm, min_docfreq = 2, min_termfreq = 20, verbose = TRUE)",br(),
"tri.df <- as.data.frame(convert(tri.dfmfile, to = 'tripletlist')[2:3])",br(),
"tri.df <- tri.df %>%",br(),
"  group_by(feature) %>%",br(),
"  summarise(frequency = n()) %>%",br(),
"  arrange(desc(frequency))",br(),
strong("#split the word column"),br(),
"ngram3 <- data.frame(do.call('rbind', strsplit(as.character(tri.df$feature),'_',fixed=TRUE)))",br(),
"names(ngram3) <- c('w1','w2','w3')",br(),
"ngram3$freq <- tri.df$frequency",br(),
"rm(tri.dfmfile, tri.dfm, trigram)",br(),
br(),
strong("# 4-Grams"),br(),
"four.dfm <- dfm(fourgram)",br(),
"four.dfmfile <- dfm_trim(four.dfm, min_docfreq = 2, min_termfreq = 20, verbose = TRUE)",br(),
"four.df <- as.data.frame(convert(four.dfmfile, to = 'tripletlist')[2:3])",br(),
"four.df <- four.df %>%",br(),
"  group_by(feature) %>%",br(),
"  summarise(frequency = n()) %>%",br(),
"  arrange(desc(frequency))",br(),
strong("#split the word column"),br(),
"ngram4 <- data.frame(do.call('rbind', strsplit(as.character(four.df$feature),'_',fixed=TRUE)))",br(),
"names(ngram4) <- c('w1','w2','w3','w4')",br(),
"ngram4$freq <- four.df$frequency",br(),
"rm(four.dfmfile, four.dfm, fourgram)",br(),
br(),
strong("#save ngrams as Rdata files for further use."),br(),
"saveRDS(ngram4, 'ngram4.rds')",br(),
"saveRDS(ngram3, 'ngram3.rds')",br(),
"saveRDS(ngram2, 'ngram2.rds')",br(),
"saveRDS(ngram1, 'ngram1.rds')"),
                )
    ),
    hr(),
    h4("Made by Teo Lo Piparo",
       a(p("Click here to jump to the Github Repo."), href="https://github.com/Teolone88/datasciencecoursera/tree/master/Data%20Science%20Capstone")
    )
))
